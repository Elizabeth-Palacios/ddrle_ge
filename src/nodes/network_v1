#!/usr/bin/env python
#################################################################################
#Copyright 2022 Elizabeth
#
#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#distributed under the License is distributed on an "AS IS" BASIS,
#See the License for the specific language governing permissions and
#limitations under the License.
#################################################################################
import rospy
import os
import json
import numpy as np
import random
import time
import pickle
import tensorflow as tf
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from environment_v1 import Behaviour
from collections import deque
from std_msgs.msg import Float32MultiArray
from robot_v1 import Robot
from keras.models import Sequential, load_model
from keras.optimizers import Adam,RMSprop
from keras.layers import Dense, Dropout, Activation
from keras.callbacks import History, TerminateOnNaN, EarlyStopping, ReduceLROnPlateau
import keras
import tensorflow as tf

gpu_options = tf.GPUOptions(allow_growth=True)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
keras.backend.tensorflow_backend.set_session(sess)

from numba import cuda
tf.logging.set_verbosity(tf.logging.ERROR)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class ReinforcementNetwork(object):
    """
    Algorithm DDRLE-GE
    """
    def __init__(self,state_size,action_size,episodes):

        self.dirPath                 = os.path.dirname(os.path.realpath(__file__))
        self.dirPath                 = self.dirPath.replace('ddrl_ge/src/nodes', 'ddrl_ge/src/save_model/environment_')
        self.state_size              = state_size
        self.action_size             = action_size
        self.episodes                = episodes
        self.episode_step            = 6000
        self.target_update           = 2000
        self.discount_factor         = 0.99
        self.learning_rate           = 0.00030
        self.batch_size              = 96
        self.train_start             = 96
        self.Pa                      = 0
        self.Pbest                   = 0.01
        self.Pbest_max               = 0.95
        self.increase_factor         = 0.97
        self.size_layer_1            = 512
        self.size_layer_2            = 512
        self.size_layer_3            = 256
        self.size_layer_4            = 20
        self.reward_max              = 0
        self.tau                     = 0.1
        self.target_value            = 0
        self.dropout                 = 0.2
        self.lim_q_s                 = 0.95
        self.lim_q_i                 = 0.25
        self.start_or                = 0.186
        self.lim_train               = 0.186
        self.memory_D                = deque(maxlen=100000)
        self.memory_GT               = deque(maxlen=100000)
        self.memory_EPS              = deque(maxlen=100000)
        self.load_model              = False
        self.load_episode            = 0
        self.loss                    = 'mse'
        self.activation_output       = 'linear'
        self.activation_layer        = 'relu'
        self.kernel_initializador = 'lecun_uniform'
        self.q_model              = self.q_network()
        self.target_model         = self.target_network()
        self.tf_rewards           = 0

        if self.load_model:
            self.q_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_q_model'+".h5").get_weights())
            self.target_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_target_model'+".h5").get_weights())
            self.Pa,self.Pbest= self.load_mode()
        else:
            self.q_model              = self.q_network()
            self.target_model         = self.target_network()

    def load_mode(self):
        '''
        The desired model is loaded
        '''
        with open(self.dirPath+str(self.load_episode)+'.json') as outfile:
            param = json.load(outfile)
            Pa    = param.get('Pa')
            Pbest = param.get('Pbest')
        return  Pa, Pbest

    def q_network(self):
        '''
        This network will be trained all the time and will be in charge of choosing the action to be executed.
        '''
        q_model = Sequential()
        q_model.add(Dense(self.size_layer_1, input_shape=(self.state_size,), activation= self.activation_layer, kernel_initializer = self.kernel_initializador))
        q_model.add(Dense(self.size_layer_2, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        q_model.add(Dropout(self.dropout))
        q_model.add(Dense(self.action_size, kernel_initializer=self.kernel_initializador))
        q_model.add(Activation(self.activation_output))
        q_model.compile(loss=self.loss, optimizer=RMSprop(lr=self.learning_rate, rho=0.9,  epsilon=1e-08, decay=0.0), metrics=['acc'])
        return q_model

    def target_network(self):
        '''
        This network will be updated based on q_network and will be in charge of evaluating the executed action.
        '''
        target_model = Sequential()
        target_model.add(Dense(self.size_layer_1, input_shape=(self.state_size,), activation= self.activation_layer, kernel_initializer = self.kernel_initializador))
        target_model.add(Dense(self.size_layer_2, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        target_model.add(Dropout(self.dropout))
        target_model.add(Dense(self.action_size, kernel_initializer=self.kernel_initializador))
        target_model.add(Activation(self.activation_output))
        target_model.compile(loss=self.loss, optimizer=RMSprop(lr=self.learning_rate, rho=0.9, epsilon=1e-08, decay=0.0), metrics=['acc'])
        return target_model

    def get_Qvalue(self, reward, next_target, done):
        '''
        Calculate the value of q-value taking into account the reward and the discount factor.
        '''
        if done:
            return reward
        else:
            return reward + self.discount_factor * next_target

    def update_target_network(self):
        '''
        Update target network
        '''
        q_model_theta      = self.q_model.get_weights()
        target_model_theta = self.target_model.get_weights()
        counter            = 0
        for q_weight, target_weight in zip(q_model_theta, target_model_theta):
            target_weight               = target_weight *(1-self.tau) + q_weight*self.tau
            target_model_theta[counter] = target_weight
            counter                    += 1
        rospy.loginfo("UPDATE TARGET NETWORK")
        return self.target_model.set_weights(target_model_theta)

    def get_Pa(self):
        '''
        Calculates the probability by "Semi-Uniform Distributed Exploration"
        If Pbes=0 purely random exploration.
        If Pbest=1 pure exploitation.
        '''
        self.Pa = self.Pbest + ((1.0-self.Pbest)/self.action_size)
        return self.Pa

    def get_action(self,state):
        '''
        Action is determined based on directed knowledge, hybrid knowledge
        or autonomous knowledge
        '''
        n2 = np.random.rand()
        n3 = np.random.rand()

        if self.Pa <= self.lim_q_i: # choose based on the rules
            self.q_value = np.zeros(self.action_size)
            action       = None
            evolve_rule  = True

        elif self.lim_q_s>self.Pa >self.lim_q_i:
            if n3 > self.Pa:  # choose based on the rules
                self.q_value = np.zeros(self.action_size)
                action       = None
                evolve_rule  = True

            else: # choose based on the q network

                self.q_value = self.q_model.predict(state.reshape(1,len(state)))
                action      = np.argmax(self.q_value[0][:5])
                evolve_rule = False

        else:
            if n2 <= self.Pa: # choose based on the q network
                self.q_value = self.q_model.predict(state.reshape(1,len(state)))
                action = np.argmax(self.q_value[0][:5])
                evolve_rule = False

            else: # choose based on the probability of q network
                self.q_value = self.q_model.predict(state.reshape(1, len(state)))
                q_value = self.q_value[0][:-1]
                mask=~(np.arange(self.action_size -1)==np.argmax(q_value))
                q_value=q_value[mask]
                q_value= q_value-min(q_value)
                q_value = q_value/sum(q_value)
                action = np.random.choice(np.arange(self.action_size-1)[mask],p=q_value)
                evolve_rule = False

        return action, evolve_rule

    def append_D(self, state, action, reward, next_state, done):
        '''
        Memory with all the data collected by the agent.
        '''
        self.memory_D.append((state, action, reward, next_state, done))

    def append_EPS(self, state,action, reward, next_state, done):
        '''
        Memory with the data collected by the agent in each episode.
        '''
        self.memory_EPS.append((state,action, reward, next_state, done))

    def winning_state(self):
        '''
        When the agent reaches the target, the temporary memory is copied into the
        main memory depending on the average reward.
        '''
        all_rewards = map(lambda x: x[2],self.memory_EPS)
        reward_aver = np.mean(all_rewards)
        if reward_aver > self.reward_max:
            self.reward_max = reward_aver
            self.memory_D.extend(self.memory_EPS)
            self.memory_D.extend(self.memory_EPS)
            self.memory_D.extend(self.memory_EPS)
            self.memory_EPS.clear()
            rospy.loginfo("Winning State with reward_max !!!")
        else:
            self.memory_EPS.clear()
            rospy.loginfo("Normal Win !!!")

    def best_state(self):
        '''
        When the agent reaches the goal with the best time, the temporary
        memory is copied into the main memory depending on the best time.
        '''
        self.memory_GT.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_EPS.clear()
        rospy.loginfo("Great Time !!!")

    def experience_replay(self):
        '''
        Based on probability choose random samples or continuous samples with
        the best average rewards
        '''
        batch_save   = []
        max_rew_save = []
        for i in range(1):
            num_2 = random.randrange(0,len(self.memory_D)-int(self.batch_size/8.0))
            if len(self.memory_GT)>2:
                mini_batch1 = deque(np.array(self.memory_D)[num_2:num_2+int(self.batch_size/8.0)-2])
            else:
                mini_batch1 = deque(np.array(self.memory_D)[num_2:num_2+int(self.batch_size/8.0)])
            batch_save.append(mini_batch1)
            all_rewards = np.array(map(lambda x: x[2],mini_batch1))
            max_reward = np.sum(all_rewards)
            max_rew_save.append(max_reward)
        idx_max_ = np.argmax(max_rew_save)

        if len(self.memory_GT)>2:
            id_gt =random.sample(self.memory_GT, 2)
            id_ra =random.sample(self.memory_D,int(self.batch_size-(self.batch_size/8.0)))
            mini_batch = batch_save[idx_max_]
            mini_batch.extend(id_gt)
            mini_batch.extend(id_ra)
        else:
            id_ra =random.sample(self.memory_D,int(self.batch_size-(self.batch_size/8.0)))
            mini_batch = batch_save[idx_max_]
            mini_batch.extend(id_ra)
        return mini_batch

    def train_model(self):
        '''
        Train model
        '''
        mini_batch =  self.experience_replay()
        X_batch = np.empty((0, self.state_size), dtype=np.float64)
        Y_batch = np.empty((0, self.action_size), dtype=np.float64)

        for i in range(self.batch_size):
            states               = mini_batch[i][0]
            actions              = mini_batch[i][1]
            rewards              = mini_batch[i][2]
            next_states          = mini_batch[i][3]
            dones                = mini_batch[i][4]
            q_value              = self.q_model.predict(states.reshape(1, len(states)))
            self.q_value         = q_value
            next_q_value         = self.q_model.predict(next_states.reshape(1, len(next_states)))
            id_max_1             = np.argmax(next_q_value)
            next_target          = self.target_model.predict(next_states.reshape(1, len(next_states)))
            next_t               = next_target[0][id_max_1]
            self.target_value    = next_t
            next_q_value         = self.get_Qvalue(rewards, next_t, dones)
            X_batch              = np.append(X_batch, np.array([states.copy()]), axis=0)
            Y_sample             = q_value.copy()
            Y_sample[0][actions] = next_q_value
            Y_batch              = np.append(Y_batch, np.array([Y_sample[0]]), axis=0)

            if dones:
                X_batch          = np.append(X_batch, np.array([next_states.copy()]), axis=0)
                Y_batch          = np.append(Y_batch, np.array([[rewards] * self.action_size]), axis=0)
        reduce_lr    = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=0, min_lr=0.0001)
        result       = self.q_model.fit(X_batch, Y_batch, batch_size=self.batch_size, epochs=1, verbose=0,callbacks=[reduce_lr])

if __name__ == '__main__':
    rospy.init_node('network_v1')
    state_size   = 28
    action_size  = 6
    episodes     = 19000
    ep           = 0
    global_step  = 0
    env          = Behaviour()
    robot        = Robot(action_size,env)
    agent        = ReinforcementNetwork(state_size, action_size,episodes)
    start_time   = time.time()
    agent.get_Pa()
    with tf.device('/GPU:0'):
        for e in range(agent.load_episode +1, episodes):

            ep               += 1
            done              = False
            finish            = False
            robot.reset()
            env.reset_gazebo()
            time.sleep(0.5)
            state_initial     = env.reset(robot.robot_position_x, robot.robot_position_y)
            state, _          = robot.state
            state             = np.asarray(state)
            score             = 0
            cont              = 0
            robot.diff_time   = 0.20
            for t in range(agent.episode_step):
                robot.step           = t
                action,evolve_rule   = agent.get_action(state)
                if agent.Pa > agent.start_or:
                    if robot.process =="collision":
                        break
                    else:
                        pass

                if evolve_rule or robot.process =="collision":
                    action = robot.evolve()
                    robot.perform_action(action)
                    next_state, reward, done = robot.next_values(action)
                else:
                    robot.perform_action(action)
                    print("action nn: ", action)
                    next_state, reward, done = robot.next_values(action)
                agent.tf_rewards = reward
                next_state[0:24] = next_state[0:24]/3.5
                next_state[25]   = next_state[25]/env._goal_distance_initial
                next_state[26]   = next_state[26]/3.5
                next_state[27]   = next_state[27]/24.0

                agent.append_D(state, action, reward, next_state, done)
                agent.append_EPS(state, action, reward, next_state, done)

                if not os.path.exists(agent.dirPath+"_map"+'.txt'):
                    with open(agent.dirPath+"_map"+'.txt','a') as outfile:
                        outfile.write("state".rjust(150," ")+"   "+"robot_x".rjust(10," ")\
                        +"   "+"robot_y".rjust(10," ")+"   "+"target_x".rjust(10," ")\
                        +"   "+"target_y".rjust(10," "))

                with open(agent.dirPath +"_map"+ '.txt', 'a') as outfile:
                    outfile.write(' '.join(np.array(state).astype(str))+"   "\
                    +"{: .3e}".format(robot.robot_position_x)+"   "\
                    +"{: .3e}".format(robot.robot_position_y)+"   "\
                    +"{: .3e}".format(env.goal_x)+"   "+"{: .3e}".format(env.goal_y) +"\n")

                if not os.path.exists(agent.dirPath +"_value"+'.txt'):
                    with open(agent.dirPath +"_value"+'.txt', 'a') as outfile:
                        outfile.write("step".rjust(8," ")+ "   "+"episode".rjust(8," ")\
                        + "   "+"a".rjust(1," ")+"   "+"   "+"reward".rjust(10," ")\
                        +"   "+"score".rjust(10," ")+"   "+"robot_x".rjust(10," ")\
                        +"   "+"robot_y".rjust(10," ")+"  "+"goal_x".rjust(10," ")\
                        +"   "+"goal_y".rjust(10," ") +"   " +"e_r".rjust(1," ")\
                        +"   "+"q_value".rjust(8," ")+"   "+"time".rjust(10," ")\
                        +"   " +"win".rjust(4," ")+"   " +"fail".rjust(4," ")\
                        +"   " +"ep".rjust(10," ")\
                        +"   "+"t_h".rjust(2," ")+"   "+"t_m".rjust(2," ")\
                        +"   "+"t_s".rjust(2," ")+"\n")

                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
                with open(agent.dirPath +"_value"+'.txt', 'a') as outfile:
                    outfile.write("{:8d}".format(t)+"   "+"{:8d}".format(ep)\
                    +"   "+str(action)+"   "+"   "+"{: .3e}".format(reward)\
                    +"   "+"{: .3e}".format(score)+"   "+"{: .3e}".format(robot.robot_position_x)\
                    +"   "+"{: .3e}".format(robot.robot_position_y)+"   "+"{: .3e}".format(env.goal_x) \
                    +"   "+"{: .3e}".format(env.goal_y) +"   " +str(int(evolve_rule))\
                    +"   "+"{: .3e}".format(np.max(agent.q_value))+"   "+"{: .3e}".format(env.best_time)\
                    +"   " +str(int(env.get_goalbox))+"   " +str(int(done))\
                    +"   "+"{: .3e}".format(agent.Pa)\
                    +"   "+"{:8d}".format(h)+"   "+"{:02d}".format(m) \
                    +"   "+"{:02d}".format(s) +"\n")

                if  env.get_goalbox ==True:
                    if env.best_time > 0.85:
                        agent.best_state()
                    else:
                        agent.winning_state()

                if  env.get_goalbox ==True or done==True:
                    m, s = divmod(int(time.time() - start_time), 60)
                    h, m = divmod(m, 60)
                    if not os.path.exists(agent.dirPath +"_time_goal"+'.txt'):
                        with open(agent.dirPath +"_time_goal"+'.txt', 'a') as outfile:
                            outfile.write("episode".rjust(8," ")+ "   "+"   "+"m".rjust(10," ")\
                            +"   "+"robot_x".rjust(10," ")\
                            +"   "+"robot_y".rjust(10," ")+"  "+"goal_x".rjust(10," ")\
                            +"   "+"goal_y".rjust(10," ") \
                            +"   "+"g".rjust(4," ")\
                            +"   " +"f".rjust(4," ")\
                            +"   "+"t_h".rjust(4," ")+"   "+"t_m".rjust(4," ")\
                            +"   "+"t_s".rjust(4," ")+"   " +"ep".rjust(4," ")+"\n")

                    with open(agent.dirPath +"_time_goal"+'.txt', 'a') as outfile:
                        outfile.write("{:8d}".format(ep)+"   "+"{:8d}".format(len(agent.memory_D))\
                        +"   "+"{: .3e}".format(robot.robot_position_x)\
                        +"   "+"{: .3e}".format(robot.robot_position_y)+"   "+"{: .3e}".format(env.goal_x) \
                        +"   "+"{: .3e}".format(env.goal_y) \
                        +"   " +str(int(env.get_goalbox))+"   " +str(int(done))\
                        +"   "+"{:8d}".format(h)+"   "+"{:02d}".format(m) \
                        +"   "+"{:02d}".format(s)\
                        +"   "+"{: .3e}".format(agent.Pa)+"\n")

                    if agent.Pa <agent.start_or:
                        # Calculate the distance to the target from the agent's last position
                        env.get_goalbox = False
                        env.goal_x, env.goal_y = env.target_position.getPosition(True, delete=True)
                        robot.last_heading=list()
                        env.get_Distance_goal(robot.robot_position_x,robot.robot_position_y)
                    else:
                        # Returns to the agent's origin position and calculates the distance to the target
                        env.get_goalbox        = False
                        robot.reset()
                        env.reset_gazebo()
                        robot.last_heading     = list()
                        env.goal_x, env.goal_y = env.target_position.getPosition(True, delete=True)
                        env.get_Distance_goal(robot.robot_position_x,robot.robot_position_y)

                if  len(agent.memory_D) > (agent.train_start):
                    agent.train_model()
                score += reward
                state  = next_state

                if e % 5 == 0:
                    agent.q_model.save(agent.dirPath + str(e)+'_q_model' +'.h5')
                    agent.target_model.save(agent.dirPath + str(e) +'_target_model'+'.h5')
                    param_keys = ['Pa','Pbest']
                    param_values = [agent.Pa, agent.Pbest]
                    param_dictionary = dict(zip(param_keys, param_values))
                    with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                        json.dump(param_dictionary, outfile)
                if t >= 500:
                    rospy.loginfo("Time out!!")
                    finish=True
                    done = True

                if done:
                    agent.update_target_network()

                    if finish:
                        finish = False
                        break

                    if evolve_rule:
                        robot.process="collision"
                        ep +=1
                        cont+=1

                        if agent.Pbest     < agent.Pbest_max:
                            agent.Pbest   /= agent.increase_factor
                        elif agent.Pbest   > agent.Pbest_max:
                            agent.Pbest    = agent.Pbest_max
                            agent.Pa       = agent.Pbest_max
                        else:
                            agent.Pbest    = agent.Pbest
                        agent.get_Pa()
                        robot.last_heading = list()

                        env.reset(robot.robot_position_x,robot.robot_position_y)

                        if cont > 20:
                            cont=0
                            break
                    else:
                        break

                global_step += 1
                if global_step % agent.target_update == 0:
                    agent.update_target_network()

            if agent.Pbest   < agent.Pbest_max:
                agent.Pbest /= agent.increase_factor

            elif agent.Pbest > agent.Pbest_max:
                agent.Pbest  = agent.Pbest_max
                agent.Pa     = agent.Pbest_max
            else:
                agent.Pbest  = agent.Pbest

            agent.get_Pa()
